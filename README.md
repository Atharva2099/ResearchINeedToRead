# ResearchINeedToRead

A curated collection of influential research papers in AI and Large Language Models (LLMs).

## Table of Contents
- [Foundational Papers](#foundational-papers)
- [Attention and Transformers](#attention-and-transformers)
- [Large Language Models](#large-language-models)
- [Training Techniques](#training-techniques)
- [Neural Network Architecture](#neural-network-architecture)
- [Optimization and Efficiency](#optimization-and-efficiency)

## Foundational Papers
- **The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain**
  - Foundational paper introducing the perceptron model
  - [Read Paper](papers/THE%20PERCEPTRON-%20A%20PROBABILISTIC%20MODEL%20FOR%20INFORMATION%20STORAGE%20AND%20ORGANIZATION%20IN%20THE%20BRAIN.pdf)

- **Learning Representations by Back-propagating Errors**
  - Fundamental paper on backpropagation algorithm
  - [Read Paper](papers/%28Backprop%29%20Learning%20representations%20by%20back-propagating%20errors%20.pdf)

- **Deep Learning**
  - Comprehensive overview of deep learning techniques
  - [Read Paper](papers/Deep%20learning.pdf)

## Attention and Transformers
- **Attention Is All You Need**
  - Original transformer architecture paper
  - [Read Paper](papers/Attention%20Is%20All%20You%20Need.pdf)

- **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**
  - Introduces BERT model and bidirectional training
  - [Read Paper](papers/BERT-%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding.pdf)

## Large Language Models
- **GPT-3: Language Models are Few-Shot Learners**
  - Breakthrough paper on large-scale language models and few-shot learning
  - [Read Paper](papers/%28GPT-3%29%20Language%20Models%20are%20Few-Shot%20Learners.pdf)

- **InstructGPT: Training Language Models to Follow Instructions with Human Feedback**
  - Details on alignment and instruction-following in LLMs
  - [Read Paper](papers/%28InstructGPT%29%20Training%20language%20models%20to%20follow%20instructions%20with%20human%20feedback.pdf)

- **Scaling Instruction-Finetuned Language Models**
  - Insights on scaling instruction-tuning
  - [Read Paper](papers/Scaling%20Instruction-Finetuned%20Language%20Models.pdf)

## Training Techniques
- **RLHF: Deep Reinforcement Learning from Human Preferences**
  - Framework for learning from human feedback
  - [Read Paper](papers/%28RLHF%29%20Deep%20Reinforcement%20Learning%20from%20Human%20Preferences.pdf)

- **LoRA: Low-Rank Adaptation of Large Language Models**
  - Efficient fine-tuning method
  - [Read Paper](papers/LORA-%20LOW-RANK%20ADAPTATION%20OF%20LARGE%20LANGUAGE%20MODELS.pdf)

- **Distilling the Knowledge in a Neural Network**
  - Knowledge distillation techniques
  - [Read Paper](papers/Distilling%20the%20Knowledge%20in%20a%20Neural%20Network.pdf)

## Neural Network Architecture
- **Sequence to Sequence Learning with Neural Networks**
  - Fundamental seq2seq architecture
  - [Read Paper](papers/%28Seq-2-Seq%29%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.pdf)

- **Neural Machine Translation by Jointly Learning to Align and Translate**
  - Early work on attention mechanisms
  - [Read Paper](papers/NEURAL%20MACHINE%20TRANSLATION%20BY%20JOINTLY%20LEARNING%20TO%20ALIGN%20AND%20TRANSLATE.pdf)

- **Word2Vec: Distributed Representations of Words and Phrases**
  - Word embedding techniques
  - [Read Paper](papers/%28Word-2-Vec%29%20Distributed%20Representations%20of%20Words%20and%20Phrases%20and%20their%20Compositionality.pdf)

## Optimization and Efficiency
- **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness**
  - Optimization for attention computation
  - [Read Paper](papers/FlashAttention-%20Fast%20and%20Memory-Efficient%20Exact%20Attention%20with%20IO-Awareness.pdf)

- **Quantized Neural Networks**
  - Network quantization techniques
  - [Read Paper](papers/Quantized%20Neural%20Networks-%20Training%20Neural%20Networks%20with%20Low%20Precision%20Weights%20and%20Activations.pdf)

- **Test-Time Compute: Scaling LLM Test-Time Compute**
  - Optimization of computational resources
  - [Read Paper](papers/%28Test-Time%20Compute%29%20Scaling%20LLM%20Test-Time%20Compute%20Optimally%20can%20be%20More%20Effective%20than%20Scaling%20Model%20Parameters.pdf)

## Recent Developments
- **DeepSeek Technical Reports**
  - DeepSeek-R1: Reasoning Capability in LLMs
  - [Read Paper](papers/DeepSeek-R1-%20Incentivizing%20Reasoning%20Capability%20in%20LLMs%20via%20Reinforcement%20Learning.pdf)
  - DeepSeek-V3 Technical Report
  - [Read Paper](papers/DeepSeek-V3%20Technical%20Report.pdf)

- **SigLIP: Sigmoid Loss for Language Image Pre-Training**
  - Advanced loss function for multimodal learning
  - [Read Paper](papers/%28SigLIP%29%20Sigmoid%20Loss%20for%20Language%20Image%20Pre-Training.pdf)

## Contributing
Feel free to suggest additional papers or improvements to this collection by creating an issue or pull request.
